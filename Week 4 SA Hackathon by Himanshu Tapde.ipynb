{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e14f4f-4db1-49f8-8a23-7f0d3e3a8323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded and global random state set.\n",
      "\n",
      "--- Section 1: Data Loading ---\n",
      "Datasets loaded successfully from 'MultipleFiles' directory.\n",
      "\n",
      "Train Data Head:\n",
      "       SEQN  RIAGENDR  PAQ605  BMXBMI  LBXGLU  DIQ010  LBXGLT  LBXIN age_group\n",
      "0  73564.0       2.0     2.0    35.7   110.0     2.0   150.0  14.91     Adult\n",
      "1  73568.0       2.0     2.0    20.3    89.0     2.0    80.0   3.85     Adult\n",
      "2  73576.0       1.0     2.0    23.2    89.0     2.0    68.0   6.14     Adult\n",
      "3  73577.0       1.0     2.0    28.9   104.0     NaN    84.0  16.15     Adult\n",
      "4  73580.0       2.0     1.0    35.9   103.0     2.0    81.0  10.92     Adult\n",
      "\n",
      "Test Data Head:\n",
      "       SEQN  RIAGENDR  PAQ605  BMXBMI  LBXGLU  DIQ010  LBXGLT  LBXIN\n",
      "0  77017.0       1.0     1.0    32.2    96.0     2.0   135.0  15.11\n",
      "1  75580.0       2.0     2.0    26.3   100.0     2.0   141.0  15.26\n",
      "2  73820.0       1.0     2.0    28.6   107.0     2.0   136.0   8.82\n",
      "3  80489.0       2.0     1.0    22.1    93.0     2.0   111.0  12.13\n",
      "4  82047.0       1.0     1.0    24.7    91.0     2.0   105.0   3.12\n",
      "\n",
      "Train Data Shape: (1966, 9)\n",
      "Test Data Shape: (312, 8)\n",
      "\n",
      "Dropped 14 rows from training data due to missing 'age_group'.\n",
      "New Train Data Shape after target NaN drop: (1952, 9)\n",
      "\n",
      "--- Section 2: Exploratory Data Analysis (EDA) ---\n",
      "\n",
      "Missing values in Train Data:\n",
      "SEQN         12\n",
      "RIAGENDR     18\n",
      "PAQ605       13\n",
      "BMXBMI       18\n",
      "LBXGLU       13\n",
      "DIQ010       18\n",
      "LBXGLT       11\n",
      "LBXIN         9\n",
      "age_group     0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in Test Data:\n",
      "SEQN        2\n",
      "RIAGENDR    2\n",
      "PAQ605      1\n",
      "BMXBMI      1\n",
      "LBXGLU      1\n",
      "DIQ010      1\n",
      "LBXGLT      2\n",
      "LBXIN       1\n",
      "dtype: int64\n",
      "\n",
      "Value counts for 'age_group' in Train Data:\n",
      "age_group\n",
      "Adult     1638\n",
      "Senior     314\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values for RIAGENDR in Train Data: [ 2.  1. nan]\n",
      "Unique values for RIAGENDR in Test Data: [ 1.  2. nan]\n",
      "\n",
      "Unique values for PAQ605 in Train Data: [ 2.  1. nan  7.]\n",
      "Unique values for PAQ605 in Test Data: [ 1.  2. nan]\n",
      "\n",
      "Unique values for DIQ010 in Train Data: [ 2. nan  1.  3.]\n",
      "Unique values for DIQ010 in Test Data: [ 2.  1. nan  3.]\n",
      "\n",
      "--- Section 3: Data Preprocessing & Feature Engineering ---\n",
      "Target variable 'age_group' encoded (Adult=0, Senior=1).\n",
      "Feature Engineering applied to both datasets.\n",
      "Preprocessing pipelines defined for all features (original + engineered).\n",
      "\n",
      "--- Section 4: Model Selection & Pipeline Creation ---\n",
      "Full ML pipeline created.\n",
      "\n",
      "--- Section 5: Hyperparameter Tuning (RandomizedSearchCV) ---\n",
      "Starting RandomizedSearchCV with 150 iterations and 5-fold CV...\n",
      "Fitting 5 folds for each of 150 candidates, totalling 750 fits\n",
      "[LightGBM] [Info] Number of positive: 314, number of negative: 1638\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001610 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1551\n",
      "[LightGBM] [Info] Number of data points in the train set: 1952, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.160861 -> initscore=-1.651838\n",
      "[LightGBM] [Info] Start training from score -1.651838\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "Best parameters found: {'classifier__subsample': 0.8, 'classifier__reg_lambda': 0.1, 'classifier__reg_alpha': 1, 'classifier__num_leaves': 20, 'classifier__n_estimators': 300, 'classifier__min_child_samples': 20, 'classifier__max_depth': 12, 'classifier__learning_rate': 0.05, 'classifier__colsample_bytree': 0.7, 'classifier__boosting_type': 'dart'}\n",
      "Best cross-validation accuracy: 0.8412\n",
      "Hyperparameter tuning complete. Best model selected.\n",
      "\n",
      "--- Section 6: Final Evaluation on Training Data ---\n",
      "Accuracy on full training data: 0.8847\n",
      "ROC AUC on full training data: 0.9233\n",
      "\n",
      "Classification Report on full training data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.99      0.94      1638\n",
      "           1       0.91      0.32      0.47       314\n",
      "\n",
      "    accuracy                           0.88      1952\n",
      "   macro avg       0.90      0.65      0.70      1952\n",
      "weighted avg       0.89      0.88      0.86      1952\n",
      "\n",
      "\n",
      "--- Section 7: Making Predictions on Test Data ---\n",
      "Predictions generated for the test set.\n",
      "\n",
      "--- Section 8: Prepare Submission File ---\n",
      "\n",
      "Submission file created successfully at: submission.csv\n",
      "Submission Head:\n",
      "    age_group\n",
      "0          0\n",
      "1          0\n",
      "2          0\n",
      "3          0\n",
      "4          0\n",
      "Submission Value Counts:\n",
      " age_group\n",
      "0    298\n",
      "1     14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Workflow Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- Section 0: Initial Setup and Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# FIX: Import to enable experimental IterativeImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import gc # Garbage collection for memory management\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set a global random state for reproducibility\n",
    "GLOBAL_RANDOM_STATE = 42\n",
    "\n",
    "print(\"Libraries loaded and global random state set.\")\n",
    "\n",
    "# --- Section 1: Data Loading ---\n",
    "print(\"\\n--- Section 1: Data Loading ---\")\n",
    "# Adjust paths if your files are in a 'MultipleFiles' subfolder\n",
    "try:\n",
    "    train_df = pd.read_csv('C:/Users/Janvi/myproject/Summer Analytics 2025/Train_Data.csv')\n",
    "    test_df = pd.read_csv('C:/Users/Janvi/myproject/Summer Analytics 2025/Test_Data.csv')\n",
    "    print(\"Datasets loaded successfully from 'MultipleFiles' directory.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Files not found in 'MultipleFiles' directory. Trying current directory...\")\n",
    "    train_df = pd.read_csv('Train_Data.csv')\n",
    "    test_df = pd.read_csv('Test_Data.csv')\n",
    "    print(\"Datasets loaded successfully from current directory.\")\n",
    "\n",
    "# Display initial data info\n",
    "print(\"\\nTrain Data Head:\\n\", train_df.head())\n",
    "print(\"\\nTest Data Head:\\n\", test_df.head())\n",
    "print(\"\\nTrain Data Shape:\", train_df.shape)\n",
    "print(\"Test Data Shape:\", test_df.shape)\n",
    "\n",
    "# --- FIX START ---\n",
    "# Handle missing values in the target variable 'age_group'\n",
    "# Drop rows where 'age_group' is NaN\n",
    "initial_train_rows = train_df.shape[0]\n",
    "train_df.dropna(subset=['age_group'], inplace=True)\n",
    "rows_after_drop = train_df.shape[0]\n",
    "print(f\"\\nDropped {initial_train_rows - rows_after_drop} rows from training data due to missing 'age_group'.\")\n",
    "print(f\"New Train Data Shape after target NaN drop: {train_df.shape}\")\n",
    "# --- FIX END ---\n",
    "\n",
    "\n",
    "# --- Section 2: Exploratory Data Analysis (EDA) & Initial Observations ---\n",
    "# This section is crucial for understanding data quality and guiding preprocessing/FE.\n",
    "print(\"\\n--- Section 2: Exploratory Data Analysis (EDA) ---\")\n",
    "\n",
    "print(\"\\nMissing values in Train Data:\")\n",
    "print(train_df.isnull().sum()) # Check again after dropping target NaNs\n",
    "print(\"\\nMissing values in Test Data:\")\n",
    "print(test_df.isnull().sum())\n",
    "\n",
    "print(\"\\nValue counts for 'age_group' in Train Data:\")\n",
    "print(train_df['age_group'].value_counts())\n",
    "\n",
    "# Check unique values for categorical-like columns to confirm their nature\n",
    "# These columns have limited discrete values and might be better treated as categorical\n",
    "categorical_like_cols_initial = ['RIAGENDR', 'PAQ605', 'DIQ010']\n",
    "for col in categorical_like_cols_initial:\n",
    "    print(f\"\\nUnique values for {col} in Train Data: {train_df[col].unique()}\")\n",
    "    print(f\"Unique values for {col} in Test Data: {test_df[col].unique()}\")\n",
    "\n",
    "# --- Section 3: Data Preprocessing & Feature Engineering ---\n",
    "print(\"\\n--- Section 3: Data Preprocessing & Feature Engineering ---\")\n",
    "\n",
    "# 3.1 Target Variable Encoding (for training data only)\n",
    "# Map 'Adult' to 0 and 'Senior' to 1 as per instructions\n",
    "age_group_mapping = {'Adult': 0, 'Senior': 1}\n",
    "train_df['age_group'] = train_df['age_group'].map(age_group_mapping)\n",
    "print(\"Target variable 'age_group' encoded (Adult=0, Senior=1).\")\n",
    "\n",
    "# 3.2 Define Feature Engineering Function\n",
    "# This function will be applied to both training and test sets consistently.\n",
    "def apply_feature_engineering(df):\n",
    "    df_copy = df.copy() # Work on a copy to avoid SettingWithCopyWarning\n",
    "\n",
    "    # Convert relevant columns to numeric, coercing errors to NaN\n",
    "    # This is crucial before any arithmetic operations or binning\n",
    "    for col in ['BMXBMI', 'LBXGLU', 'LBXGLT', 'LBXIN']:\n",
    "        df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')\n",
    "\n",
    "    # --- Feature Engineering based on domain knowledge and common practices ---\n",
    "\n",
    "    # 1. BMI Categories (as discussed, good for capturing non-linear effects)\n",
    "    # Labels are numerical for easier processing later (e.g., OneHotEncoder)\n",
    "    df_copy['BMI_Category_FE'] = pd.cut(df_copy['BMXBMI'],\n",
    "                                       bins=[0, 18.5, 24.9, 29.9, 34.9, 39.9, np.inf],\n",
    "                                       labels=[0, 1, 2, 3, 4, 5], # Underweight, Normal, Overweight, Obese I, II, III\n",
    "                                       right=True,\n",
    "                                       include_lowest=True)\n",
    "\n",
    "    # 2. Glucose-Insulin Interaction (potential indicator of insulin resistance)\n",
    "    # Handle potential NaNs in LBXGLU or LBXIN before multiplication\n",
    "    df_copy['GLU_IN_Interaction_FE'] = df_copy['LBXGLU'] * df_copy['LBXIN']\n",
    "\n",
    "    # 3. Glucose-BMI Ratio (how glucose levels relate to body mass)\n",
    "    # Handle potential division by zero or NaN in BMXBMI\n",
    "    df_copy['GLU_BMI_Ratio_FE'] = df_copy['LBXGLU'] / df_copy['BMXBMI']\n",
    "\n",
    "    # 4. Glucose Tolerance Index (LBXGLT / LBXGLU)\n",
    "    # This ratio can be indicative of how well the body processes glucose\n",
    "    df_copy['GLT_GLU_Ratio_FE'] = df_copy['LBXGLT'] / df_copy['LBXGLU']\n",
    "\n",
    "    # 5. Activity Level Indicator (PAQ605) - Convert to binary if not already\n",
    "    # Assuming 1.0 is active, 2.0 is not active. Convert to 0/1.\n",
    "    # Handle potential NaNs in PAQ605\n",
    "    df_copy['PAQ605_Active_FE'] = df_copy['PAQ605'].apply(lambda x: 1 if x == 1.0 else (0 if x == 2.0 else np.nan))\n",
    "\n",
    "    # 6. Diabetes Status Indicator (DIQ010) - Convert to binary\n",
    "    # Assuming 1.0 is Yes, 2.0 is No, 3.0 is Borderline. Convert to 0/1 for 'has diabetes'\n",
    "    # Handle potential NaNs in DIQ010\n",
    "    df_copy['DIQ010_Diabetes_FE'] = df_copy['DIQ010'].apply(lambda x: 1 if x == 1.0 else (0 if x in [2.0, 3.0] else np.nan))\n",
    "\n",
    "    # 7. Gender (RIAGENDR) - Convert to binary (Male=0, Female=1)\n",
    "    # This ensures consistency and proper encoding if not already done by LabelEncoder\n",
    "    df_copy['RIAGENDR_Binary_FE'] = df_copy['RIAGENDR'].apply(lambda x: 0 if x == 1.0 else (1 if x == 2.0 else np.nan))\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "# Apply FE to both training features (X) and test features (X_test)\n",
    "X = apply_feature_engineering(train_df.drop(['SEQN', 'age_group'], axis=1))\n",
    "y = train_df['age_group'] # Target variable\n",
    "X_test = apply_feature_engineering(test_df.drop('SEQN', axis=1))\n",
    "\n",
    "print(\"Feature Engineering applied to both datasets.\")\n",
    "\n",
    "# 3.3 Define Column Types for Preprocessing Pipelines\n",
    "# Original numerical features (will be imputed and scaled)\n",
    "original_numerical_features = ['BMXBMI', 'LBXGLU', 'LBXGLT', 'LBXIN']\n",
    "\n",
    "# Original categorical-like features (will be imputed and one-hot encoded)\n",
    "original_categorical_features = ['RIAGENDR', 'PAQ605', 'DIQ010']\n",
    "\n",
    "# Newly engineered numerical features (will be imputed and scaled)\n",
    "engineered_numerical_features = [\n",
    "    'GLU_IN_Interaction_FE',\n",
    "    'GLU_BMI_Ratio_FE',\n",
    "    'GLT_GLU_Ratio_FE'\n",
    "]\n",
    "\n",
    "# Newly engineered categorical features (will be imputed and one-hot encoded)\n",
    "engineered_categorical_features = [\n",
    "    'BMI_Category_FE',\n",
    "    'PAQ605_Active_FE',\n",
    "    'DIQ010_Diabetes_FE',\n",
    "    'RIAGENDR_Binary_FE'\n",
    "]\n",
    "\n",
    "# Combine lists for the ColumnTransformer\n",
    "all_numerical_features = original_numerical_features + engineered_numerical_features\n",
    "all_categorical_features = original_categorical_features + engineered_categorical_features\n",
    "\n",
    "# 3.4 Define Preprocessing Steps (Pipelines for Numerical and Categorical)\n",
    "# Numerical pipeline: Impute with MICE, then scale\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', IterativeImputer(random_state=GLOBAL_RANDOM_STATE, max_iter=10, initial_strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline: Impute with most frequent, then one-hot encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')) # handle_unknown='ignore' for unseen categories in test set\n",
    "])\n",
    "\n",
    "# Create a ColumnTransformer to apply different transformations to different columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, all_numerical_features),\n",
    "        ('cat', categorical_transformer, all_categorical_features)\n",
    "    ],\n",
    "    remainder='drop' # Drop any columns not explicitly transformed (like original SEQN)\n",
    ")\n",
    "\n",
    "print(\"Preprocessing pipelines defined for all features (original + engineered).\")\n",
    "\n",
    "# --- Section 4: Model Selection & Pipeline Creation ---\n",
    "print(\"\\n--- Section 4: Model Selection & Pipeline Creation ---\")\n",
    "\n",
    "# Using LightGBM Classifier, known for high performance and speed\n",
    "# Set objective to 'binary' for binary classification\n",
    "lgbm_model = lgb.LGBMClassifier(objective='binary', random_state=GLOBAL_RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "# Create the full pipeline including preprocessing and the model\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lgbm_model)\n",
    "])\n",
    "\n",
    "print(\"Full ML pipeline created.\")\n",
    "\n",
    "# --- Section 5: Hyperparameter Tuning (RandomizedSearchCV with StratifiedKFold) ---\n",
    "print(\"\\n--- Section 5: Hyperparameter Tuning (RandomizedSearchCV) ---\")\n",
    "\n",
    "# Define parameter distribution for RandomizedSearchCV\n",
    "# These ranges are carefully chosen based on common LGBM best practices.\n",
    "param_distributions = {\n",
    "    'classifier__n_estimators': [200, 300, 500, 700, 1000], # Number of boosting rounds\n",
    "    'classifier__learning_rate': [0.01, 0.03, 0.05, 0.07, 0.1], # Step size shrinkage\n",
    "    'classifier__num_leaves': [20, 31, 40, 50, 60, 80], # Max tree leaves for base learners\n",
    "    'classifier__max_depth': [-1, 5, 8, 10, 12, 15], # Max tree depth (-1 means no limit)\n",
    "    'classifier__min_child_samples': [10, 20, 30, 40, 50, 60], # Minimum data in a leaf\n",
    "    'classifier__subsample': [0.7, 0.8, 0.9, 1.0], # Subsample ratio of the training instance\n",
    "    'classifier__colsample_bytree': [0.7, 0.8, 0.9, 1.0], # Subsample ratio of columns when constructing each tree\n",
    "    'classifier__reg_alpha': [0, 0.01, 0.1, 0.5, 1, 2], # L1 regularization term\n",
    "    'classifier__reg_lambda': [0, 0.01, 0.1, 0.5, 1, 2], # L2 regularization term\n",
    "    'classifier__boosting_type': ['gbdt', 'dart'] # Gradient Boosting Decision Tree or DART\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold for cross-validation to maintain class balance in each fold\n",
    "# This is important for classification tasks, especially with potential class imbalance.\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=GLOBAL_RANDOM_STATE)\n",
    "\n",
    "# RandomizedSearchCV setup\n",
    "# n_iter: number of parameter settings that are sampled. Increase for more thorough search.\n",
    "# scoring: 'accuracy' is good for balanced datasets, 'roc_auc' for imbalanced. Sticking to accuracy as per problem.\n",
    "random_search = RandomizedSearchCV(\n",
    "    full_pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=150, # Increased iterations for a more exhaustive search (adjust based on time/resources)\n",
    "    cv=cv_strategy, # Use StratifiedKFold\n",
    "    scoring='accuracy',\n",
    "    random_state=GLOBAL_RANDOM_STATE,\n",
    "    n_jobs=-1, # Use all available CPU cores\n",
    "    verbose=2, # Increased verbosity to see progress\n",
    "    error_score='raise' # Raise errors to debug issues during search\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV on the full training data (X, y)\n",
    "print(f\"Starting RandomizedSearchCV with {random_search.n_iter} iterations and {cv_strategy.n_splits}-fold CV...\")\n",
    "random_search.fit(X, y)\n",
    "\n",
    "print(f\"\\nBest parameters found: {random_search.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# Get the best model from the search\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Clean up memory after search\n",
    "del random_search\n",
    "gc.collect()\n",
    "\n",
    "print(\"Hyperparameter tuning complete. Best model selected.\")\n",
    "\n",
    "# --- Section 6: Final Evaluation on Training Data (for insight, not for submission) ---\n",
    "print(\"\\n--- Section 6: Final Evaluation on Training Data ---\")\n",
    "# This gives an idea of how well the best model performs on the data it was trained/validated on.\n",
    "y_pred_train = best_model.predict(X)\n",
    "train_accuracy = accuracy_score(y, y_pred_train)\n",
    "train_roc_auc = roc_auc_score(y, best_model.predict_proba(X)[:, 1])\n",
    "\n",
    "print(f\"Accuracy on full training data: {train_accuracy:.4f}\")\n",
    "print(f\"ROC AUC on full training data: {train_roc_auc:.4f}\")\n",
    "print(\"\\nClassification Report on full training data:\")\n",
    "print(classification_report(y, y_pred_train))\n",
    "\n",
    "# --- Section 7: Make Predictions on Test Data ---\n",
    "print(\"\\n--- Section 7: Making Predictions on Test Data ---\")\n",
    "\n",
    "# Make predictions using the best model\n",
    "test_predictions = best_model.predict(X_test)\n",
    "\n",
    "print(\"Predictions generated for the test set.\")\n",
    "\n",
    "# --- Section 8: Prepare Submission File ---\n",
    "print(\"\\n--- Section 8: Prepare Submission File ---\")\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({'age_group': test_predictions})\n",
    "\n",
    "# Ensure the age_group column is integer type (0 or 1) as required by sample_submission.csv\n",
    "submission_df['age_group'] = submission_df['age_group'].astype(int)\n",
    "\n",
    "# Save the submission file\n",
    "submission_file_path = 'submission.csv'\n",
    "submission_df.to_csv(submission_file_path, index=False)\n",
    "\n",
    "print(f\"\\nSubmission file created successfully at: {submission_file_path}\")\n",
    "print(\"Submission Head:\\n\", submission_df.head())\n",
    "print(\"Submission Value Counts:\\n\", submission_df['age_group'].value_counts())\n",
    "\n",
    "print(\"\\n--- Workflow Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12edcd8d-3c36-4985-a74a-cffc1480a532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
